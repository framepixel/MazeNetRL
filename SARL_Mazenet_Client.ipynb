{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81afe7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SARL_Env_v3 import MazeEnv\n",
    "\n",
    "from stable_baselines3 import A2C, PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8af958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "875af410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from JavalessClient import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4245b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = Client(\"manager\", \"127.0.0.1\", 9888, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcaf207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.login_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a149af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, Box, Tuple, MultiBinary, MultiDiscrete, Dict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75569102",
   "metadata": {},
   "outputs": [],
   "source": [
    "menv = MazeEnv()#.to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f9ce8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5da17a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "path = 'Training/Logs'\n",
    "model = PPO('MultiInputPolicy',\n",
    "            menv,\n",
    "            verbose=1,\n",
    "            tensorboard_log=path,\n",
    "            device=\"cuda\",\n",
    "#             learning_rate=0.007,\n",
    "#             n_steps=10,\n",
    "#             gamma=0.7,\n",
    "            \n",
    "           )\n",
    "checkpoint_callback = CheckpointCallback( save_freq=2048,\n",
    "                                          save_path=\"./saved_ppo_v2/\",\n",
    "                                          name_prefix=\"ppo_v2_model_\",\n",
    "                                          save_replay_buffer=True,\n",
    "                                          save_vecnormalize=True,)\n",
    "eval_callback = EvalCallback(menv, best_model_save_path=\"./best_ppo_v2/\",\n",
    "                             log_path=\"./logs/\", eval_freq=2000,\n",
    "                             deterministic=True, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3de1783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs\\PPO_71\n",
      "Eval num_timesteps=2000, episode_reward=-548.80 +/- 386.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -549     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -572     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 196      |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-900.00 +/- 489.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -900         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0117304595 |\n",
      "|    clip_fraction        | 0.112        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.9         |\n",
      "|    n_updates            | 4520         |\n",
      "|    policy_gradient_loss | -0.0109      |\n",
      "|    value_loss           | 46.6         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -501     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 386      |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-679.60 +/- 461.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -680        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027620342 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 4530        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 23.8        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -454     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 567      |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-493.80 +/- 403.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -494        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018275889 |\n",
      "|    clip_fraction        | 0.0993      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.975      |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 4540        |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 4.9         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -421     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 757      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-500.00 +/- 400.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -500        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023690704 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.4        |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.26        |\n",
      "|    n_updates            | 4550        |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    value_loss           | 8.14        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -408     |\n",
      "| time/              |          |\n",
      "|    fps             | 9        |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 1027     |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-796.80 +/- 416.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -797        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014820573 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.24        |\n",
      "|    n_updates            | 4560        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 7.41        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -391     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 1217     |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-524.80 +/- 390.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -525        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011798164 |\n",
      "|    clip_fraction        | 0.0986      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.862      |\n",
      "|    explained_variance   | -3.58e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.89        |\n",
      "|    n_updates            | 4570        |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 4.81        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -383     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 1432     |\n",
      "|    total_timesteps | 14336    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-301.60 +/- 3.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -302        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020181794 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -3.58e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.12        |\n",
      "|    n_updates            | 4580        |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 4.78        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -377     |\n",
      "| time/              |          |\n",
      "|    fps             | 9        |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 1638     |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-304.80 +/- 5.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -305        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014909244 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.35        |\n",
      "|    n_updates            | 4590        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 12.3        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -427     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 1836     |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-996.00 +/- 365.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -996        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007918511 |\n",
      "|    clip_fraction        | 0.0271      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.24       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.8        |\n",
      "|    n_updates            | 4600        |\n",
      "|    policy_gradient_loss | -0.00464    |\n",
      "|    value_loss           | 84.2        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -435     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 2016     |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-902.40 +/- 485.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -902        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016865596 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.3        |\n",
      "|    n_updates            | 4610        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 36.5        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -422     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 2217     |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-715.20 +/- 476.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -715       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 24000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02056594 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.36      |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.28       |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    value_loss           | 16.8       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -419     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 2403     |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-294.60 +/- 10.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -295       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 26000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02157067 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | -3.58e-07  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.31       |\n",
      "|    n_updates            | 4630       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    value_loss           | 4.25       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -411     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 2602     |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-727.20 +/- 469.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -727        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024963371 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.2        |\n",
      "|    n_updates            | 4640        |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 51.1        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -428     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 2785     |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-749.60 +/- 458.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -750        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013851052 |\n",
      "|    clip_fraction        | 0.0896      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 4650        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 21.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -421     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 2964     |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-1018.60 +/- 346.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -1.02e+03    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0113148615 |\n",
      "|    clip_fraction        | 0.0578       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.602       |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.5          |\n",
      "|    n_updates            | 4660         |\n",
      "|    policy_gradient_loss | -0.0154      |\n",
      "|    value_loss           | 5.51         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -416     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 3146     |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-900.00 +/- 489.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -900        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011994928 |\n",
      "|    clip_fraction        | 0.0659      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.606      |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.92        |\n",
      "|    n_updates            | 4670        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 6.68        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -411     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 3333     |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n",
      "<DisconnectMessageData>\n",
      "    <name>player_1</name>\n",
      "    <errortypeCode>TIMEOUT</errortypeCode>\n",
      "</DisconnectMessageData>\n",
      "\n",
      "Eval num_timesteps=36000, episode_reward=-513.80 +/- 393.72\n",
      "Episode length: 849.00 +/- 302.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 849         |\n",
      "|    mean_reward          | -514        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014173655 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.73        |\n",
      "|    n_updates            | 4680        |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 16.5        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -408     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 3535     |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-404.00 +/- 200.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -404         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 38000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0133504905 |\n",
      "|    clip_fraction        | 0.113        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.4          |\n",
      "|    n_updates            | 4690         |\n",
      "|    policy_gradient_loss | -0.00899     |\n",
      "|    value_loss           | 4.61         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -403     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 3782     |\n",
      "|    total_timesteps | 38912    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-305.60 +/- 7.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -306        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017186973 |\n",
      "|    clip_fraction        | 0.0943      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.74       |\n",
      "|    explained_variance   | -2.38e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.53        |\n",
      "|    n_updates            | 4700        |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 3.22        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -399     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 3975     |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-636.00 +/- 406.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -636       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 42000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02536079 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.1       |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.66       |\n",
      "|    n_updates            | 4710       |\n",
      "|    policy_gradient_loss | -0.026     |\n",
      "|    value_loss           | 17.8       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -404     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 4162     |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-504.80 +/- 397.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -505       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 44000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01655118 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.23      |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 10.7       |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    value_loss           | 24.3       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -403     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 4348     |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-515.20 +/- 392.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -515        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017258758 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.26        |\n",
      "|    n_updates            | 4730        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 12.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -401     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 4558     |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-305.00 +/- 22.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -305        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013773844 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.54        |\n",
      "|    n_updates            | 4740        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    value_loss           | 8.36        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -397     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 4753     |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-692.20 +/- 491.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -692        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035743512 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.3         |\n",
      "|    n_updates            | 4750        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 9.68        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -398     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 4934     |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-592.00 +/- 391.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -592       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 52000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02154043 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.917     |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.01       |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | -0.0262    |\n",
      "|    value_loss           | 8.1        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -394     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 5174     |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-304.80 +/- 2.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -305        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017187448 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.23        |\n",
      "|    n_updates            | 4770        |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 29.5        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -394     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 5363     |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-504.00 +/- 396.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -504       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 56000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01159735 |\n",
      "|    clip_fraction        | 0.0585     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.674     |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.37       |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    value_loss           | 6.01       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -391     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 5577     |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-604.80 +/- 396.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -605        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022714619 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.4        |\n",
      "|    n_updates            | 4790        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    value_loss           | 22          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -389     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 5768     |\n",
      "|    total_timesteps | 59392    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-900.00 +/- 489.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -900        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013181509 |\n",
      "|    clip_fraction        | 0.096       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.656      |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.52        |\n",
      "|    n_updates            | 4800        |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    value_loss           | 9.91        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -387     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 5948     |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-732.00 +/- 465.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -732        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007776406 |\n",
      "|    clip_fraction        | 0.0644      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.747      |\n",
      "|    explained_variance   | -2.38e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.505       |\n",
      "|    n_updates            | 4810        |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 4.02        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -384     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 6144     |\n",
      "|    total_timesteps | 63488    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-992.80 +/- 394.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -993        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 64000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018443309 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.87        |\n",
      "|    n_updates            | 4820        |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 9           |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -385     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 6337     |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-902.40 +/- 483.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -902        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024450734 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.7        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 4830        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 24.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -384     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 6556     |\n",
      "|    total_timesteps | 67584    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-519.40 +/- 391.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -519        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028872848 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.16       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.99        |\n",
      "|    n_updates            | 4840        |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 14.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -383     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 6738     |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-526.40 +/- 389.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -526       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 70000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01953739 |\n",
      "|    clip_fraction        | 0.0693     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.603     |\n",
      "|    explained_variance   | -1.19e-07  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.51       |\n",
      "|    n_updates            | 4850       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    value_loss           | 3.99       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -381     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 6925     |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-707.20 +/- 484.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -707        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020713795 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.73        |\n",
      "|    n_updates            | 4860        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 5.58        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -380     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 7108     |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-538.40 +/- 383.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -538        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013594154 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 4870        |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 15.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -378     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 7338     |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-470.40 +/- 197.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -470        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 76000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036448658 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.92        |\n",
      "|    n_updates            | 4880        |\n",
      "|    policy_gradient_loss | -0.00944    |\n",
      "|    value_loss           | 4.7         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -376     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 7522     |\n",
      "|    total_timesteps | 77824    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-904.80 +/- 482.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | -905      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 78000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0113491 |\n",
      "|    clip_fraction        | 0.0507    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.615    |\n",
      "|    explained_variance   | -1.19e-07 |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.28      |\n",
      "|    n_updates            | 4890      |\n",
      "|    policy_gradient_loss | -0.0123   |\n",
      "|    value_loss           | 6.63      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -375     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 7701     |\n",
      "|    total_timesteps | 79872    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-496.20 +/- 402.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -496        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020174434 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.6        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.6        |\n",
      "|    n_updates            | 4900        |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 35.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -377     |\n",
      "| time/              |          |\n",
      "|    fps             | 10       |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 7889     |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1e6da025340>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for step in range(int(steps/5000)):\n",
    "model.learn(total_timesteps=steps, callback=[checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3543b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_path = \"Training/Saved Models/PPO_\"+str(1000000)+\"_Mazenet_Model_v2\"\n",
    "model.save(a2c_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "622b5d5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21472\\3760656076.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea27787",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(\"./Training/Saved Models/PPO_100000_Mazenet_Model_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8503f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the first csv file into a pandas dataframe\n",
    "df1 = pd.read_csv('./con_file_2.csv')\n",
    "#df1 = df1.drop(\"Wall time\", axis=1)\n",
    "\n",
    "# Find the last value of the column containing increasing integers\n",
    "last_value = df1['Step'].iloc[-1]\n",
    "\n",
    "# Read the second csv file into another pandas dataframe\n",
    "df2 = pd.read_csv('./Training/csv_ppo_v2_4.csv')\n",
    "df2 = df2.drop(\"Wall time\", axis=1)\n",
    "\n",
    "# Modify the column containing increasing integers in the second dataframe\n",
    "start_value = last_value #+ df1[\"Step\"].iloc[0]\n",
    "#increment = 500\n",
    "df2['Step'] = start_value + df2[\"Step\"]\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "df_concatenated = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# Write the concatenated dataframe to a new csv file\n",
    "df_concatenated.to_csv('con_file_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b6edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"./Training/csv_ppo.csv\")\n",
    "\n",
    "# Remove the \"column_name\" column from the DataFrame\n",
    "df = df.drop(\"Wall time\", axis=1)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file called \"output_file.csv\"\n",
    "df.to_csv(\"csv_ppo.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = menv.reset()\n",
    "done = False  \n",
    "total_reward   = 0\n",
    "while not done:\n",
    "    #print(menv.action_space.sample())\n",
    "    obs, reward, done, trunc, info =  menv.step(menv.action_space.sample())\n",
    "    total_reward  += reward\n",
    "print('Total Reward for episode {} is {}'.format(episode, total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecff7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = MazeEnv()\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ec36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Box\n",
    "b = Box(low=0, high=1, shape=(7, 7), dtype=int).sample()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d0ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b912ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((7, 7), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3ca121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
