{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e78dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AStA\\anaconda3\\envs\\gym\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\AStA\\anaconda3\\envs\\gym\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\AStA\\anaconda3\\envs\\gym\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\AStA\\anaconda3\\envs\\gym\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Boiler plate stuff to start the module\n",
    "import jpype\n",
    "import jpype.imports\n",
    "from jpype.types import *\n",
    "import sys \n",
    "import numpy as np\n",
    "import traceback\n",
    "import random\n",
    "import Client\n",
    "import copy\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f4292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Java modules\n",
    "from java.io import IOException\n",
    "from java.net import Socket\n",
    "from java.net import UnknownHostException\n",
    "\n",
    "from javax.net.ssl import SSLSocket\n",
    "from javax.net.ssl import SSLSocketFactory\n",
    "from javax.xml.bind import JAXBException\n",
    "from javax.xml.bind import UnmarshalException\n",
    "\n",
    "from de.fhac.mazenet.server.generated import AwaitMoveMessageData\n",
    "from de.fhac.mazenet.server.generated import Errortype\n",
    "from de.fhac.mazenet.server.generated import MazeCom\n",
    "from de.fhac.mazenet.server.generated import MazeComMessagetype\n",
    "from de.fhac.mazenet.server.generated import MoveMessageData\n",
    "from de.fhac.mazenet.server.generated import BoardData\n",
    "\n",
    "from de.fhac.mazenet.server.networking import MazeComMessageFactory\n",
    "from de.fhac.mazenet.server.networking import XmlInputStream\n",
    "from de.fhac.mazenet.server.networking import XmlOutputStream\n",
    "\n",
    "from de.fhac.mazenet.server.game import Board\n",
    "from de.fhac.mazenet.server.game import Position\n",
    "from de.fhac.mazenet.server.game import Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afca3ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"127.0.0.1\"\n",
    "PORT = 9888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb73702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1c20ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import pydirectinput\n",
    "import cv2\n",
    "import pytesseract\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ce59d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "from pettingzoo import AECEnv, ParallelEnv\n",
    "from pettingzoo.utils import agent_selector, parallel_to_aec, wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dca6b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_range = np.arange(1,6,2)\n",
    "potentialShiftMoves =  []\n",
    "potentialShiftMovesPos =  []\n",
    "for i in ls_range:\n",
    "    potentialShiftMoves.append((0, i))\n",
    "    potentialShiftMoves.append((6, i))\n",
    "    potentialShiftMoves.append((i, 0))\n",
    "    potentialShiftMoves.append((i, 6))\n",
    "    potentialShiftMovesPos.append(Position(0, i))\n",
    "    potentialShiftMovesPos.append(Position(6, i))\n",
    "    potentialShiftMovesPos.append(Position(i, 0))\n",
    "    potentialShiftMovesPos.append(Position(i, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ffaf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_Client:\n",
    "    def __init__(self, name, client=None):\n",
    "        self.name = name\n",
    "        self.client = client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3de6add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MARL_Env(AECEnv):\n",
    "    \n",
    "    metadata = {\"render_modes\": [\"human\"], \"name\": \"rps_v2\"}\n",
    "\n",
    "    def __init__(self, render_mode=None, num_players=4):\n",
    "        #self.manager = Client.Client(\"manager\", HOST, PORT, True)\n",
    "        #self.manager.login_manager()\n",
    "        \n",
    "        self.num_players = num_players\n",
    "        self.possible_agents = [Agent_Client(name=\"player_\" + str(r)) for r in range(self.num_players)]\n",
    "        self.agent_name_mapping = dict(\n",
    "            zip(self.possible_agents, list(range(len(self.possible_agents))))\n",
    "        )\n",
    "\n",
    "        # Gym spaces are defined and documented here: https://gym.openai.com/docs/#spaces\n",
    "        self._action_spaces = {agent: MultiDiscrete([12, 4, 7, 7], dtype=np.int8) for agent in self.possible_agents}\n",
    "        self._observation_spaces = {\n",
    "            agent: Tuple((Box(low=0, high=6, shape=(1,2), dtype=np.uint8),\n",
    "                           Box(low=0, high=6, shape=(1,2), dtype=np.uint8)\n",
    "                           ))  for agent in self.possible_agents\n",
    "        }\n",
    "        self.render_mode = render_mode\n",
    "        self.player_pos = {agent: Box(low=0, high=6, shape=(1,2)) for agent in self.possible_agents}\n",
    "        self.next_treasure_pos = {agent: Box(low=0, high=6, shape=(1,2)) for agent in self.possible_agents}\n",
    "        self.is_done = False\n",
    "        \n",
    "\n",
    "    # this cache ensures that same space object is returned for the same agent\n",
    "    # allows action space seeding to work as expected\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        # Gym spaces are defined and documented here: https://gym.openai.com/docs/#spaces\n",
    "        return Tuple((Box(low=0, high=6, shape=(1,2), dtype=np.uint8),\n",
    "                           Box(low=0, high=6, shape=(1,2), dtype=np.uint8)\n",
    "                           ))\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return MultiDiscrete([12, 4, 7, 7], dtype=np.int8)\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"\n",
    "        Observe should return the observation of the specified agent. This function\n",
    "        should return a sane observation (though not necessarily the most up to date possible)\n",
    "        at any time after reset() is called.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close should release any graphical displays, subprocesses, network connections\n",
    "        or any other environment data which should not be kept around after the\n",
    "        user is no longer using the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, return_info=False, options=None):\n",
    "        \"\"\"\n",
    "        Reset needs to initialize the following attributes\n",
    "        - agents\n",
    "        - rewards\n",
    "        - _cumulative_rewards\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - infos\n",
    "        - agent_selection\n",
    "        And must set up the environment so that render(), step(), and observe()\n",
    "        can be called without issues.\n",
    "        Here it sets up the state dictionary which is used by step() and the observations dictionary which is used by step() and observe()\n",
    "        \"\"\"\n",
    "        #self.manager.stop_game(self.num_players)\n",
    "        #self.manager.start_game(self.num_players)\n",
    "        \n",
    "        pydirectinput.click(x=1050, y=150)\n",
    "        # start\n",
    "        pydirectinput.click(x=950, y=150)\n",
    "        time.sleep(1)\n",
    "        for agent in self.possible_agents:\n",
    "            while True:\n",
    "                agent.client = Client.Client(agent.name, HOST, PORT, True)\n",
    "                try:\n",
    "                    login = MazeComMessageFactory.createLoginMessage(agent.name)\n",
    "                    agent.client.out.write(login)\n",
    "                    receivedMazeCom = agent.client.in_.readMazeCom()\n",
    "                    if(receivedMazeCom.getMessagetype() == MazeComMessagetype.LOGINREPLY):\n",
    "                        newid = receivedMazeCom.getLoginReplyMessage().getNewID()\n",
    "                        print(\"newid:\", newid)\n",
    "                        agent.client.setId(newid)\n",
    "                        break\n",
    "                    else:\n",
    "                        print(receivedMazeCom.getAcceptMessage().getErrortypeCode())\n",
    "                except:\n",
    "                    print(\"failed login :)\")\n",
    "        #self.possible_agents = [Client.Client(\"player_\" + str(r), HOST, PORT, True) for r in range(self.num_players+1)]\n",
    "        #self.possible_agents = self.possible_agents[1:]\n",
    "        \n",
    "        self.is_done = False\n",
    "        for i in self.possible_agents:\n",
    "            print(\"id:\", i.client.id_)\n",
    "        #print(\"agents:\", self.agents)\n",
    "        self.possible_agents = sorted(self.possible_agents, key=operator.attrgetter('client.id_'))\n",
    "        for i in self.possible_agents:\n",
    "            print(\"id:\", i.client.id_)\n",
    "            \n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {agent: 0 for agent in self.agents}\n",
    "        self._cumulative_rewards = {agent: 0 for agent in self.agents}\n",
    "        self.terminations = {agent: False for agent in self.agents}\n",
    "        self.truncations = {agent: False for agent in self.agents}\n",
    "        self.infos = {agent: {} for agent in self.agents}\n",
    "        self.state = {agent: None for agent in self.agents}\n",
    "        self.observations = {agent: None for agent in self.agents}\n",
    "        self.num_moves = 0\n",
    "        \n",
    "        \"\"\"\n",
    "        Our agent_selector utility allows easy cyclic stepping through the agents list.\n",
    "        \"\"\"\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "        print(\"end reset\")\n",
    "            \n",
    "\n",
    "                \n",
    "    def moeglicheOrientierungen(self, rot, card):\n",
    "        card_ = Card(card)\n",
    "        openings = card_.getOpenings()\n",
    "        if(card.getShape() == Card.CardShape.I):\n",
    "            openings.setBottom(not openings.isBottom())\n",
    "            openings.setLeft(not openings.isLeft())\n",
    "            openings.setRight(not openings.isRight())\n",
    "            openings.setTop(not openings.isTop())\n",
    "            return  Card(card_)\n",
    "        else:\n",
    "            if(rot == 0):\n",
    "                return Card(card.getShape(), Card.Orientation.D0, card.getTreasure())\n",
    "            if(rot == 1):\n",
    "                return Card(card.getShape(), Card.Orientation.D90, card.getTreasure())\n",
    "            if(rot == 2):\n",
    "                return Card(card.getShape(), Card.Orientation.D180, card.getTreasure())\n",
    "            if(rot == 3):\n",
    "                return Card(card.getShape(), Card.Orientation.D270, card.getTreasure())\n",
    "        return Card(card.getShape(), Card.Orientation.D0, card.getTreasure())\n",
    "    \n",
    "    def Move(self, awaitMove, action):\n",
    "        reward = 0\n",
    "        boardData = awaitMove.getBoard()\n",
    "        treasure = awaitMove.getTreasureToFindNext()\n",
    "        board = Board(boardData)\n",
    "        playerPosition = board.findPlayer(self.agent_selection.client.getId())\n",
    "        psm = potentialShiftMoves[action[0]]\n",
    "        position = Position(psm[0], psm[1])\n",
    "        \n",
    "        potentialMove = MoveMessageData()\n",
    "        psmp = copy.copy(potentialShiftMovesPos)\n",
    "        if(position == board.getForbidden()):\n",
    "            #print(\"forbidden shift position, random shift position used, reward decreased\")\n",
    "            reward -= 10\n",
    "            try:\n",
    "                psmp.remove(position)\n",
    "            except Exception:\n",
    "                print(\"element not in list, skipping\")\n",
    "            potentialMove.setShiftPosition(psmp[0])\n",
    "        else:\n",
    "            potentialMove.setShiftPosition(position)\n",
    "        \n",
    "        treasurePositionData = board.findTreasure(treasure)\n",
    "        \n",
    "        orientedShiftCard = self.moeglicheOrientierungen(action[1], board.getShiftCard())\n",
    "        \n",
    "        potentialMove.setShiftCard(orientedShiftCard)\n",
    "        potentialMove.setNewPinPos(playerPosition)\n",
    "\n",
    "        boardNext = board.fakeShift(potentialMove)\n",
    "        treasurePositionData = boardNext.findTreasure(treasure)\n",
    "        \n",
    "        new_player_pos = boardNext.findPlayer(self.agent_selection.client.getId())\n",
    "        reachablePositions = boardNext.getAllReachablePositions(new_player_pos)\n",
    "        rp = np.array(reachablePositions)\n",
    "        chosen_pos_obj = Position(action[2], action[3])\n",
    "        chosen_pos_arr = np.array(action[2], action[3])\n",
    "        if(treasurePositionData == None):\n",
    "            #print(\"no treasure? skipping\")\n",
    "            potentialMove.setNewPinPos(new_player_pos)\n",
    "            self.next_treasure_pos[self.agent_selection] = treasurePositionData\n",
    "            self.player_pos[self.agent_selection] = new_player_pos\n",
    "            return potentialMove, reward\n",
    "\n",
    "        if(chosen_pos_obj not in rp):\n",
    "            #print(\"cannot reach position, reward decreased\")\n",
    "            potentialMove.setNewPinPos(new_player_pos)\n",
    "            self.next_treasure_pos[self.agent_selection] = treasurePositionData\n",
    "            self.player_pos[self.agent_selection] = new_player_pos\n",
    "            return potentialMove, reward - 10\n",
    "        else:\n",
    "            #print(\"treasure position data:\", treasurePositionData)\n",
    "            treasurePosition = Position(treasurePositionData)\n",
    "            #print(\"treasure pos:\", treasurePosition)\n",
    "\n",
    "            if(chosen_pos_obj == treasurePosition):\n",
    "                #print(\"straight to the treasure, reward increased!\")\n",
    "                potentialMove.setNewPinPos(treasurePosition)\n",
    "                self.next_treasure_pos[self.agent_selection] = treasurePositionData\n",
    "                self.player_pos[self.agent_selection] = treasurePosition\n",
    "                return potentialMove, reward + 20\n",
    "            else:\n",
    "                #print(\"reachable position, but no treasure\")\n",
    "                potentialMove.setNewPinPos(chosen_pos_obj)\n",
    "                self.next_treasure_pos[self.agent_selection] = treasurePositionData\n",
    "                self.player_pos[self.agent_selection] = chosen_pos_obj\n",
    "                return potentialMove, reward - 5\n",
    "\n",
    "        #print(\"Object not reachable: random move :/\")\n",
    "        potentialMove.setNewPinPos(reachablePositions[0])\n",
    "        self.next_treasure_pos[self.agent_selection] = treasurePositionData\n",
    "        self.player_pos[self.agent_selection] = reachablePositions[0]\n",
    "        return potentialMove, reward - 10\n",
    "    \n",
    "    def awaitMove(self, receivedMazeCom, action):\n",
    "        awaitMove = receivedMazeCom.getAwaitMoveMessage()\n",
    "        move, reward = self.Move(awaitMove, action)\n",
    "        mazeComToSend = MazeCom()\n",
    "        mazeComToSend.setId(self.agent_selection.client.getId())\n",
    "        mazeComToSend.setMessagetype(MazeComMessagetype.MOVE)\n",
    "        mazeComToSend.setMoveMessage(move)\n",
    "        self.agent_selection.client.out.write(mazeComToSend)\n",
    "        return reward                \n",
    "                \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        step(action) takes in an action for the current agent (specified by\n",
    "        agent_selection) and needs to update\n",
    "        - rewards\n",
    "        - _cumulative_rewards (accumulating the rewards)\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - infos\n",
    "        - agent_selection (to the next agent)\n",
    "        And any internal state used by observe() or render()\n",
    "        \"\"\"\n",
    "        print(\"begin step\")\n",
    "        if self.is_done:\n",
    "            self.terminations[self.agent_selection] = True\n",
    "        else:\n",
    "            reward = 0\n",
    "            try:\n",
    "                receivedMazeCom = self.agent_selection.client.in_.readMazeCom()\n",
    "                if(receivedMazeCom.getMessagetype() == MazeComMessagetype.LOGINREPLY):\n",
    "                    newid = receivedMazeCom.getLoginReplyMessage().getNewID()\n",
    "                    self.agent_selection.client.setId(newid)\n",
    "                elif(receivedMazeCom.getMessagetype() == MazeComMessagetype.ACCEPT):\n",
    "                    print(receivedMazeCom.getAcceptMessage().getErrortypeCode())\n",
    "                    reward += 2\n",
    "                elif (receivedMazeCom.getMessagetype() == MazeComMessagetype.DISCONNECT):\n",
    "                    print(receivedMazeCom.getDisconnectMessage().getErrortypeCode())\n",
    "                    reward -= 10\n",
    "                elif(receivedMazeCom.getMessagetype() == MazeComMessagetype.AWAITMOVE):\n",
    "                    print(action)\n",
    "                    reward += self.awaitMove(receivedMazeCom, action)\n",
    "                elif(receivedMazeCom.getMessagetype() == MazeComMessagetype.MOVEINFO):\n",
    "                    #print(\"in MoveInfo\")\n",
    "                    x = 0\n",
    "                elif(receivedMazeCom.getMessagetype() == MazeComMessagetype.WIN):\n",
    "                    print(\"You have won\")\n",
    "                    self.is_done = True\n",
    "                    reward += 100\n",
    "                else:\n",
    "                    print(\"Unknown message type: \" + receivedMazeCom.getMessagetype())\n",
    "            except Exception as e:\n",
    "                print(traceback.format_exc())\n",
    "                if str(type(e)) == \"<java class 'java.net.SocketException'>\":\n",
    "                    print(\"socket error, disconnected\")\n",
    "                    sys.exit(1)\n",
    "                elif str(type(e)) == \"<java class 'java.io.EOFException'>\":\n",
    "                    print(\"end reached, disconnecting\")\n",
    "                    sys.exit(1)\n",
    "            self.rewards[self.agent_selection] = reward\n",
    "            self._cumulative_rewards[self.agent_selection] += reward\n",
    "            self.observations[self.agent_selection] = (self.player_pos[self.agent_selection], self.next_treasure_pos[self.agent_selection])\n",
    "            if self.is_done:\n",
    "                self.terminations[self.agent_selection] = True\n",
    "            else:\n",
    "                self.terminations[self.agent_selection] = False\n",
    "            self.agent_selection = self._agent_selector.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b872acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import AgentNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9b2538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Training/Logs'\n",
    "#model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e871c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ent_coef = 0.1\n",
    "vf_coef = 0.1\n",
    "clip_coef = 0.1\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "stack_size = 4\n",
    "frame_size = (64, 64)\n",
    "max_cycles = 125\n",
    "total_episodes = 2\n",
    "\n",
    "\"\"\" ENV SETUP \"\"\"\n",
    "env = MARL_Env(num_players=4)\n",
    "num_agents = len(env.possible_agents)\n",
    "num_actions = np.prod(env.action_space(env.possible_agents[0]).nvec) # env.action_space(env.possible_agents[0]).n\n",
    "observation_size = env.observation_space(env.possible_agents[0]).shape\n",
    "\n",
    "\"\"\" LEARNER SETUP \"\"\"\n",
    "agent = AgentNN.Agent(num_actions=num_actions).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=0.001, eps=1e-5)\n",
    "\n",
    "\"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "end_step = 0\n",
    "total_episodic_return = 0\n",
    "rb_obs = torch.zeros((max_cycles, num_agents, stack_size, *frame_size)).to(device)\n",
    "rb_actions = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_logprobs = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_rewards = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_terms = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "rb_values = torch.zeros((max_cycles, num_agents)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fc73f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newid: 3\n",
      "newid: 1\n",
      "newid: 4\n",
      "newid: 2\n",
      "id: 3\n",
      "id: 1\n",
      "id: 4\n",
      "id: 2\n",
      "id: 1\n",
      "id: 2\n",
      "id: 3\n",
      "id: 4\n",
      "end reset\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# each episode has num_steps\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, max_cycles):\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# rollover the observation\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[43mAgentNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatchify_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# get action from the agent\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     actions, logprobs, _, values \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action_and_value(obs)\n",
      "File \u001b[1;32m~\\Desktop\\Bachelorarbeit\\AgentNN.py:38\u001b[0m, in \u001b[0;36mbatchify_obs\u001b[1;34m(obs, device)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"Converts PZ style observations to batch of torch arrays.\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# convert to list of np arrays\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([obs[a] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m obs], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# transpose to be (batch, channel, height, width)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m obs \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "\"\"\" TRAINING LOGIC \"\"\"\n",
    "# train for n number of episodes\n",
    "for episode in range(total_episodes):\n",
    "\n",
    "    # collect an episode\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # collect observations and convert to batch of torch tensors\n",
    "        next_obs = env.reset()\n",
    "        # reset the episodic return\n",
    "        total_episodic_return = 0\n",
    "\n",
    "        # each episode has num_steps\n",
    "        for step in range(0, max_cycles):\n",
    "\n",
    "            # rollover the observation\n",
    "            obs = AgentNN.batchify_obs(next_obs, device)\n",
    "\n",
    "            # get action from the agent\n",
    "            actions, logprobs, _, values = agent.get_action_and_value(obs)\n",
    "\n",
    "            # execute the environment and log data\n",
    "            next_obs, rewards, terms, truncs, infos = env.step(\n",
    "                AgentNN.unbatchify(actions, env)\n",
    "            )\n",
    "\n",
    "            # add to episode storage\n",
    "            rb_obs[step] = obs\n",
    "            rb_rewards[step] = AgentNN.batchify(rewards, device)\n",
    "            rb_terms[step] = AgentNN.batchify(terms, device)\n",
    "            rb_actions[step] = actions\n",
    "            rb_logprobs[step] = logprobs\n",
    "            rb_values[step] = values.flatten()\n",
    "\n",
    "            # compute episodic return\n",
    "            total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "            # if we reach termination or truncation, end\n",
    "            if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                end_step = step\n",
    "                break\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "        for t in reversed(range(end_step)):\n",
    "            delta = (\n",
    "                rb_rewards[t]\n",
    "                + gamma * rb_values[t + 1] * rb_terms[t + 1]\n",
    "                - rb_values[t]\n",
    "            )\n",
    "            rb_advantages[t] = delta + gamma * gamma * rb_advantages[t + 1]\n",
    "        rb_returns = rb_advantages + rb_values\n",
    "\n",
    "    # convert our episodes to batch of individual transitions\n",
    "    b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)\n",
    "    b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)\n",
    "    b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)\n",
    "    b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_index = np.arange(len(b_obs))\n",
    "    clip_fracs = []\n",
    "    for repeat in range(3):\n",
    "        # shuffle the indices we use to access the data\n",
    "        np.random.shuffle(b_index)\n",
    "        for start in range(0, len(b_obs), batch_size):\n",
    "            # select the indices we want to train on\n",
    "            end = start + batch_size\n",
    "            batch_index = b_index[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, value = agent.get_action_and_value(\n",
    "                b_obs[batch_index], b_actions.long()[batch_index]\n",
    "            )\n",
    "            logratio = newlogprob - b_logprobs[batch_index]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clip_fracs += [\n",
    "                    ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                ]\n",
    "\n",
    "            # normalize advantaegs\n",
    "            advantages = b_advantages[batch_index]\n",
    "            advantages = (advantages - advantages.mean()) / (\n",
    "                advantages.std() + 1e-8\n",
    "            )\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "            pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                ratio, 1 - clip_coef, 1 + clip_coef\n",
    "            )\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            value = value.flatten()\n",
    "            v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "            v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                value - b_values[batch_index],\n",
    "                -clip_coef,\n",
    "                clip_coef,\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "            v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    print(f\"Training episode {episode}\")\n",
    "    print(f\"Episodic Return: {np.mean(total_episodic_return)}\")\n",
    "    print(f\"Episode Length: {end_step}\")\n",
    "    print(\"\")\n",
    "    print(f\"Value Loss: {v_loss.item()}\")\n",
    "    print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "    print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "    print(f\"Approx KL: {approx_kl.item()}\")\n",
    "    print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "    print(f\"Explained Variance: {explained_var.item()}\")\n",
    "    print(\"\\n-------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd26df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RENDER THE POLICY \"\"\"\n",
    "env = MARL_Env(num_players=4)\n",
    "\n",
    "agent.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # render 5 episodes out\n",
    "    for episode in range(5):\n",
    "        obs = env.reset()\n",
    "        terms = [False]\n",
    "        truncs = [False]\n",
    "        while not any(terms) and not any(truncs):\n",
    "            actions, logprobs, _, values = agent.get_action_and_value(obs)\n",
    "            obs, rewards, terms, truncs, infos = env.step(AgentNN.unbatchify(actions, env))\n",
    "            obs = AgentNN.batchify_obs(obs, device)\n",
    "            terms = [terms[a] for a in terms]\n",
    "            truncs = [truncs[a] for a in truncs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a87912",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_path = \"Training/Saved Models/PPO_100_Mazenet_Model\"\n",
    "model.save(ppo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.test import api_test\n",
    "from pettingzoo.test import parallel_api_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4419fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MARL_Env(num_players=4)\n",
    "#api_test(env, num_cycles=100, verbose_progress=True)\n",
    "env.reset()\n",
    "#print(type(sorted(env.agent_iter(), key=operator.attrgetter('id_'))))\n",
    "#print(env.agent_iter())\n",
    "for agent in env.agent_iter():\n",
    "    #print(\"agent:\", agent.name)\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    action = env.action_space(agent).sample()\n",
    "    # time.sleep(3)\n",
    "    env.step(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
